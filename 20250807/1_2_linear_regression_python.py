import matplotlib.pyplot as plt

# ========================== 손실(비용) 함수를 코드로 구현 ==========================
def cost(x, y, w):
    c = 0 # 손실의 누적합
    for i in range(len(x)):
        hx = w * x[i] # 레이어(곱셈 하나당 레이어 1개)
        c += (hx - y[i]) ** 2 # 데이터 1개에 대한 손실의 제곱
    return c / len(x)

# y = ax + b
# hx = wx + b
# y = 1 * x + 0 : x랑 y 가 같을 때 손실 함수
x = [1, 2, 3]
y = [1, 2, 3]

print(cost(x, y, w=0))
print(cost(x, y, w=1))
print(cost(x, y, w=2))
#  ========================== ========================== ==========================
# ========================== 손실 함수 그래프 ==========================
def show_cost():
    for i in range(-30, 50): # range() 와 np.arange() 의 차이점 : range() 는 정수만 입력 가능, np.arange() 는 실수도 가능
        w = i / 10
        c = cost(x, y, w)

        # 그래프 그리기
        plt.plot(w, c, 'ro') # w(가중치) 에 따른 c(손실) 을 보여주는 그래프 ==> condex
    plt.show()
# show_cost()
#  ========================== ========================== ==========
# ========================== 경사 하강법 ==========================

# 미분 : 기울기, 순간변화량
#        x 축으로 1만큼 움직였을 때, y 축으로 움직인 거리

# y = 7         7=1, 7=2, 7=3   ==> 미분 상수(x와 y는 관계가 없다)
# y = x         1=1, 2=2, 3=3
# y = 2x        2=1, 4=2, 3=6
# y = (x+1)     2=1, 3=2, 4=3
# y = xz        y 를 x 또는 z 로 미분 가능

# y = x^2       1=1, 4=2, 9=3
#     2 * x^(2-1) * x미분 = 2x (미분)
# y = (x+1)^2
#     2*(x+1)^(2-1) * (x + 1)미분 = 2(x+1)

# w 가 미치는 영향을 아는 방법 : w 로 미분

def gradient_descent(x, y, w):
    g = 0
    for i in range(len(x)):
        hx = w * x[i] # 예측
        # c += (hx - y[i]) ** 2 # hx 도 전개해서 대입
        # c += (w * x[i] - y[i]) ** 2 # w 로 미분
        # c += 2 * (w * x[i] - y[i]) ** (2-1) * (w * x[i] - y[i])미분(w로)
        # c += 2 * (w * x[i] - y[i]) * (x[i] - 0) # 상수 곱하기는 의미 없으므로 제거
        g += (hx - y[i]) * x[i] # gradient descent 알고리즘 ==> 기울기 구하기
    return g / len(x)

def show_gradient():
    x = [1, 2, 3]
    y = [1, 2, 3]

    w = 5
    epochs = 10 # 반복 횟수
    for i in range(epochs):
        c = cost(x, y, w)
        g = gradient_descent(x, y, w)   # g ==> 기울기
        w -= 0.1 * g    # 0.1 ==> running rate(학습률), 클수록 빨리 학습

        # print(i, g) # 기울기가 0이되는 지점 찾기
        # print(i, w) # w 가 1.0이 되는 지점 찾기(w 가 1 일때 기울기가 0이 되므로)
        print(i, c) # 학습이 잘 되고 있는지(손실이 줄어들고 있는지) 확인
    return w

w = show_gradient() # 기울기 1.0 에 다가가는 학습
#  ========================== ========================== ==========

# 퀴즈 : w 가 1.0이 되는 코드 찾기
# 답1 : 반복 횟수를 늘림 ==> 현실적으로 사용 불가(학습 기간이 늘어남)
# 답2 : 학습률을 올림( 0.1 > 0.21 ) ==> 사용 가능(하이퍼 파라미터로 수정 가능 ==> 개발자가 사용 가능한 유일한 방법)
# 답3 : w 를 1 로 변경 ==> 사용 가능(w의 분포를 구하는 방법을 사용 - keras 내부에 구현되어 있음)

# 퀴즈 : x가 5와 7일 때의 결과를 예측하세요
x = [5, 7]
print('5일 때 답 : ', w * 5)
print('7일 때 답 : ', w * 7)

